{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"OSWiGbzV1lBl","executionInfo":{"status":"ok","timestamp":1765821852046,"user_tz":-60,"elapsed":52782,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8487760-8c8c-4cad-e331-e223f678bb2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import math\n","import csv\n","import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as T\n","\n","from dataclasses import dataclass\n","from typing import Literal\n","\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bxLLIQt41lBt","executionInfo":{"status":"ok","timestamp":1765821852047,"user_tz":-60,"elapsed":46,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"outputs":[],"source":["BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data_Mining\"\n","\n","CIFAR10_PATH = f\"{BASE_DIR}/cifar10\"\n","MNIST_PATH = f\"{BASE_DIR}/mnist\"\n","\n","TODAY = pd.Timestamp.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","RESULTS_FILE = f\"{BASE_DIR}/Results/results_2_RPE_{TODAY}.csv\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZnzxkAjw1lBu","executionInfo":{"status":"ok","timestamp":1765821852299,"user_tz":-60,"elapsed":248,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"outputs":[],"source":["# ================================================================\n","# 1. Patch embedding + CLS + simple learned absolute positions\n","# ================================================================\n","\n","class PatchEmbedding(nn.Module):\n","    \"\"\"\n","    Image -> sequence of patch embeddings via a Conv2d layer.\n","    Args:\n","        img_size: size of the input image (assumed square)\n","        patch_size: size of each patch (assumed square)\n","        in_channels: number of input channels (e.g., 3 for RGB)\n","        embed_dim: dimension of the output embeddings\n","    \"\"\"\n","\n","    def __init__(\n","        self, img_size: int, patch_size: int, in_channels: int, embed_dim: int\n","    ):\n","        super().__init__()\n","        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(\n","            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor of shape (B, C, H, W)\n","                Input batch of images.\n","        Returns:\n","            Tensor of shape (B, N, D)\n","                Sequence of patch embeddings, where:\n","                    N = number of patches\n","                    D = embedding dimension.\n","        \"\"\"\n","        x = self.proj(x)  # (B, D, H/P, W/P)\n","        x = x.flatten(2)  # (B, D, N)\n","        x = x.transpose(1, 2)  # (B, N, D)\n","        return x\n","\n","\n","class ViTInputLayer(nn.Module):\n","    \"\"\"\n","    PatchEmbedding + optional [CLS] token\n","\n","    This module converts an input image into a sequence of patch embeddings,\n","    optionally prepends a trainable [CLS] token, and adds learned positional\n","    encodings to all tokens.\n","    \"\"\"\n","\n","    def __init__(self, img_size, patch_size, in_channels, embed_dim, cls_token=True):\n","        super().__init__()\n","\n","        # Convert image into sequence of patch embeddings\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","\n","        self.num_patches = self.patch_embed.num_patches\n","\n","        # Learnable CLS token (added in front of patch tokens)\n","        self.cls_token = (\n","            nn.Parameter(torch.zeros(1, 1, embed_dim)) if cls_token else None\n","        )\n","\n","    def forward(self, x):\n","        B = x.size(0)\n","        x = self.patch_embed(x)  # (B, N, D)\n","\n","        if self.cls_token is not None:\n","            cls = self.cls_token.expand(B, -1, -1)  # match batch size\n","            x = torch.cat([cls, x], dim=1)\n","        return x\n","\n","# ================================================================\n","# 2. Multi-Head Attention variants\n","#    - Full softmax attention (baseline)\n","#    - Performer FAVOR+ (softmax approx with positive random features)\n","#    - Performer-ReLU (ReLU feature map)\n","# ================================================================\n","\n","class MultiHeadSelfAttentionFull(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim**-0.5\n","\n","        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # (B, H, N, d)\n","\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B,H,N,N)\n","        attn = F.softmax(attn_scores, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        out = torch.matmul(attn, v)  # (B,H,N,d)\n","        out = out.transpose(1, 2).reshape(B, N, D)  # (B,N,D)\n","        out = self.out_proj(out)\n","        return self.proj_drop(out)\n","\n","class PerformerAttentionFavorPlus(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        nb_features: int = 64,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","        eps: float = 1e-6,\n","        rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","        seq_len: int | None = None,\n","    ):\n","        super().__init__()\n","\n","        assert embed_dim % num_heads == 0\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.nb_features = nb_features\n","        self.eps = eps\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # Random features\n","        W = torch.randn(num_heads, nb_features, self.head_dim)\n","        self.register_buffer(\"W\", W)\n","\n","        # RPE\n","        self.rotary = None\n","        self.rpe_classic = None\n","        self.rpe_string = None\n","\n","        if rpe_type == \"rope\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for RoPE\")\n","            self.rotary = RotaryEmbedding(self.head_dim, max_seq_len=seq_len)\n","\n","        elif rpe_type == \"classic\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for classic RPE\")\n","            self.rpe_classic = ClassicRPEPerformer(seq_len)\n","\n","        elif rpe_type == \"string\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for STRING RPE\")\n","            self.rpe_string = CirculantSTRING(seq_len, self.head_dim)\n","\n","    def _favor_feature_map(self, x):\n","        B, H, N, d_k = x.shape\n","        x_proj = torch.einsum(\"b h n d, h m d -> b h n m\", x, self.W)\n","        sq_norm = (x**2).sum(dim=-1, keepdim=True) / 2.0\n","        return torch.exp(x_proj - sq_norm) / math.sqrt(self.nb_features)\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        k = k.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        v = v.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        # RoPE\n","        if self.rotary is not None:\n","            q, k = self.rotary(q, k)\n","\n","        q_feat = self._favor_feature_map(q)\n","        k_feat = self._favor_feature_map(k)\n","\n","        kv = torch.einsum(\"b h n m, b h n d -> b h m d\", k_feat, v)\n","        k_sum = k_feat.sum(dim=2)\n","\n","        denom = torch.einsum(\"b h n m, b h m -> b h n\", q_feat, k_sum)\n","        denom = denom.unsqueeze(-1) + self.eps\n","\n","        out = torch.einsum(\"b h n m, b h m d -> b h n d\", q_feat, kv)\n","        out = out / denom  # (B,H,N,d)\n","\n","        if self.rpe_classic is not None:\n","            weight = self.rpe_classic(N, x.device)  # (N,N)\n","            out = torch.einsum(\"b h n d, n m -> b h m d\", out, weight)\n","\n","        if self.rpe_string is not None:\n","            weight = self.rpe_string(N, x.device)  # (N,N)\n","            out = torch.einsum(\"b h n d, n m -> b h m d\", out, weight)\n","\n","        out = out.permute(0, 2, 1, 3).reshape(B, N, D)  # (B,N,D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","\n","class PerformerAttentionReLU(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","        eps: float = 1e-6,\n","        rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","        seq_len: int | None = None,\n","    ):\n","        super().__init__()\n","\n","        assert embed_dim % num_heads == 0\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.eps = eps\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        # RPE\n","        self.rotary = None\n","        self.rpe_classic = None\n","        self.rpe_string = None\n","\n","        if rpe_type == \"rope\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for RoPE\")\n","            self.rotary = RotaryEmbedding(self.head_dim, max_seq_len=seq_len)\n","\n","        elif rpe_type == \"classic\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for classic RPE\")\n","            self.rpe_classic = ClassicRPEPerformer(seq_len)\n","\n","        elif rpe_type == \"string\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for STRING RPE\")\n","            self.rpe_string = CirculantSTRING(seq_len, self.head_dim)\n","\n","    def _relu_feature_map(self, x):\n","        return F.relu(x)\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        k = k.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        v = v.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        # RoPE\n","        if self.rotary is not None:\n","            q, k = self.rotary(q, k)\n","\n","        q_feat = self._relu_feature_map(q)\n","        k_feat = self._relu_feature_map(k)\n","\n","        kv = torch.einsum(\"b h n d, b h n e -> b h d e\", k_feat, v)\n","        k_sum = k_feat.sum(dim=2)\n","\n","        denom = torch.einsum(\"b h n d, b h d -> b h n\", q_feat, k_sum)\n","        denom = denom.unsqueeze(-1) + self.eps\n","\n","        out = torch.einsum(\"b h n d, b h d e -> b h n e\", q_feat, kv)\n","        out = out / denom  # (B,H,N,d)\n","\n","        # Classic RPE multiplicative\n","        if self.rpe_classic is not None:\n","            weight = self.rpe_classic(N, x.device)  # (N,N)\n","            out = torch.einsum(\"b h n e, n m -> b h m e\", out, weight)\n","\n","        # STRING RPE\n","        if self.rpe_string is not None:\n","            weight = self.rpe_string(N, x.device)  # (N,N)\n","            out = torch.einsum(\"b h n e, n m -> b h m e\", out, weight)\n","\n","        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","# ================================================================\n","# 3. Transformer block + ViT backbone\n","# ================================================================\n","\n","class MLPBlock(nn.Module):\n","    \"\"\"\n","    Standard Transformer feedforward block:\n","    Linear -> GELU -> Linear (with optional dropout).\n","\n","    Position-wise feedforward network that expands and compresses each token\n","    to increase the transformer's expressive capacity.\n","    \"\"\"\n","\n","    def __init__(self, embed_dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n","        super().__init__()\n","        hidden_dim = int(embed_dim * mlp_ratio)\n","\n","        self.fc1 = nn.Linear(embed_dim, hidden_dim)  # expansion layer\n","        self.act = nn.GELU()  # non-linearity\n","        self.fc2 = nn.Linear(hidden_dim, embed_dim)  # projection back to D\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)  # (B, N, hidden_dim)\n","        x = self.act(x)  # GELU activation\n","        x = self.drop(x)  # dropout after activation\n","        x = self.fc2(x)  # back to (B, N, D)\n","        x = self.drop(x)  # optional dropout on output\n","        return x\n","\n","class TransformerEncoderBlock(nn.Module):\n","  \"\"\"\n","  Pre-LN Transformer block with selectable attention type\n","  \"\"\"\n","\n","  def __init__(\n","      self,\n","      embed_dim: int,\n","      num_heads: int,\n","      mlp_ratio: float = 4.0,\n","      attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","      nb_features: int = 64,\n","      drop: float = 0.0,\n","      attn_drop: float = 0.0,\n","      rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","      seq_len: int | None = None,\n","  ):\n","      super().__init__()\n","\n","      self.norm1 = nn.LayerNorm(embed_dim)\n","      self.norm2 = nn.LayerNorm(embed_dim)\n","\n","      if attn_type == \"full\":\n","          self.attn = MultiHeadSelfAttentionFull(\n","              embed_dim,\n","              num_heads,\n","              attn_drop,\n","              drop,\n","          )\n","\n","      elif attn_type == \"favor+\":\n","          self.attn = PerformerAttentionFavorPlus(\n","              embed_dim,\n","              num_heads,\n","              nb_features,\n","              attn_drop,\n","              drop,\n","              rpe_type=rpe_type,\n","              seq_len=seq_len,\n","          )\n","      elif attn_type == \"relu\":\n","          self.attn = PerformerAttentionReLU(\n","              embed_dim,\n","              num_heads,\n","              attn_drop=attn_drop,\n","              proj_drop=drop,\n","              rpe_type=rpe_type,\n","              seq_len=seq_len,\n","          )\n","\n","      self.mlp = MLPBlock(embed_dim, mlp_ratio, drop)\n","\n","  def forward(self, x):\n","      x = x + self.attn(self.norm1(x))\n","      x = x + self.mlp(self.norm2(x))\n","      return x\n","\n","class ViTClassifier(nn.Module):\n","    \"\"\"\n","    ViT / Performer-ViT for MNIST / CIFAR-10\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        img_size: int,\n","        patch_size: int,\n","        in_channels: int,\n","        num_classes: int,\n","        embed_dim: int = 64,\n","        depth: int = 4,\n","        num_heads: int = 4,\n","        mlp_ratio: float = 4.0,\n","        attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","        nb_features: int = 64,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","    ):\n","        super().__init__()\n","\n","        self.input_layer = ViTInputLayer(\n","            img_size, patch_size, in_channels, embed_dim, cls_token=True\n","        )\n","\n","        seq_len = 1 + (img_size // patch_size) ** 2\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                TransformerEncoderBlock(\n","                    embed_dim=embed_dim,\n","                    num_heads=num_heads,\n","                    mlp_ratio=mlp_ratio,\n","                    attn_type=attn_type,\n","                    nb_features=nb_features,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    rpe_type=rpe_type,\n","                    seq_len=seq_len,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","        cls_token = x[:, 0]\n","        return self.head(cls_token)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"nrCFGRAi1lBw","executionInfo":{"status":"error","timestamp":1765821859474,"user_tz":-60,"elapsed":7175,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}},"outputId":"bf1e13e0-26c0-42e9-bc39-5430b76ff25c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================================\n"," RUN: dataset=cifar10 | attn=favor+ | rpe=none | epochs=20\n","==============================================\n","\n","Using device: cuda\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1131902341.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============================================\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             run_training_RPE(\n\u001b[0m\u001b[1;32m    375\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0mattn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1131902341.py\u001b[0m in \u001b[0;36mrun_training_RPE\u001b[0;34m(dataset, attn_type, rpe_type, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cifar10\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cifar10_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         cfg = ViTConfig(\n\u001b[1;32m    300\u001b[0m             \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1131902341.py\u001b[0m in \u001b[0;36mget_cifar10_loaders\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Load CIFAR-10 train/test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     train_set = torchvision.datasets.CIFAR10(\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCIFAR10_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m_check_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_integrity\u001b[0;34m(fpath, md5)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcheck_md5\u001b[0;34m(fpath, md5, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcalculate_md5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mcalculate_md5\u001b[0;34m(fpath, chunk_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmd5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musedforsecurity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# ---------------------------------------------------------\n","# RPE MODULES\n","# ---------------------------------------------------------\n","\n","class ClassicRPEPerformer(nn.Module):\n","    \"\"\"\n","    Classic multiplicative RPE for Performer (Luo et al., 2021)\n","    Applied as a kernel weight: w[i,j]\n","    \"\"\"\n","\n","    def __init__(self, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.rpe = nn.Parameter(torch.zeros(seq_len))  # 1D kernel\n","\n","    def forward(self, N, device):\n","        idx = torch.arange(N, device=device)\n","\n","        # directional: (j - i) mod N\n","        rel = (idx[None, :] - idx[:, None]) % N\n","\n","        # kernel weighting\n","        weight = torch.exp(self.rpe[rel])  # (N,N)\n","        return weight\n","\n","\n","class CirculantSTRING(nn.Module):\n","    \"\"\"\n","    Circulant-STRING RPE : génère une matrice (N,N) de pondération (directionnelle).\n","    Utilisée uniquement dans les Performers.\n","    \"\"\"\n","\n","    def __init__(self, seq_len: int, head_dim: int):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.kernel = nn.Parameter(torch.zeros(seq_len))\n","        self.scale = 1.0 / math.sqrt(head_dim)\n","\n","    def forward(self, N: int, device: torch.device):\n","        if N != self.seq_len:\n","            raise ValueError(\n","                f\"CirculantSTRING requires fixed seq_len={self.seq_len}, got N={N}\"\n","            )\n","\n","        kernel_fft = torch.fft.rfft(self.kernel, n=N)  # (N_fft,)\n","\n","        eye = torch.eye(N, device=device)  # (N,N)\n","        eye_fft = torch.fft.rfft(eye, n=N, dim=-1)  # (N, N_fft)\n","\n","        out = torch.fft.irfft(eye_fft * kernel_fft[None, :], n=N)  # (N,N)\n","        return self.scale * out  # (N,N)\n","\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, head_dim: int, max_seq_len: int = 512):\n","        super().__init__()\n","        if head_dim % 2 != 0:\n","            raise ValueError(\"RoPE nécessite un head_dim pair\")\n","\n","        self.head_dim = head_dim\n","        self.max_seq_len = max_seq_len\n","\n","        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)\n","\n","    def _build_sin_cos(self, seq_len: int, device: torch.device):\n","        # positions (N,)\n","        pos = torch.arange(seq_len, dtype=torch.float32, device=device)  # (N,)\n","\n","        # freqs = (N, head_dim/2)\n","        freqs = torch.einsum(\"n,d->nd\", pos, self.inv_freq)\n","\n","        # sin/cos = (N, head_dim/2)\n","        sin = torch.sin(freqs)\n","        cos = torch.cos(freqs)\n","\n","        sin = torch.stack([sin, sin], dim=-1).reshape(seq_len, self.head_dim)\n","        cos = torch.stack([cos, cos], dim=-1).reshape(seq_len, self.head_dim)\n","\n","        # reshape (1,1,N,head_dim)\n","        return (\n","            sin.unsqueeze(0).unsqueeze(0),\n","            cos.unsqueeze(0).unsqueeze(0),\n","        )\n","\n","    @staticmethod\n","    def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n","        d = x.shape[-1]\n","        x1 = x[..., : d // 2]\n","        x2 = x[..., d // 2 :]\n","        return torch.cat([-x2, x1], dim=-1)\n","\n","    def forward(self, q, k):\n","        B, H, N, d = q.shape\n","        if N > self.max_seq_len:\n","            raise ValueError(f\"seq_len {N} > max_seq_len {self.max_seq_len}\")\n","\n","        sin, cos = self._build_sin_cos(N, q.device)\n","\n","        q_rot = (q * cos) + (self._rotate_half(q) * sin)\n","        k_rot = (k * cos) + (self._rotate_half(k) * sin)\n","        return q_rot, k_rot\n","\n","# ================================================================\n","# 4. Helper functions to build model from config\n","# ================================================================\n","@dataclass\n","class ViTConfig:\n","    img_size: int = 32\n","    patch_size: int = 4\n","    in_channels: int = 3\n","    num_classes: int = 10\n","    embed_dim: int = 64\n","    depth: int = 4\n","    num_heads: int = 4\n","    mlp_ratio: float = 4.0\n","    attn_type: str = \"favor+\"\n","    nb_features: int = 64\n","    drop: float = 0.0\n","    attn_drop: float = 0.0\n","    rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\"\n","\n","\n","def build_model(cfg: ViTConfig) -> nn.Module:\n","    return ViTClassifier(\n","        img_size=cfg.img_size,\n","        patch_size=cfg.patch_size,\n","        in_channels=cfg.in_channels,\n","        num_classes=cfg.num_classes,\n","        embed_dim=cfg.embed_dim,\n","        depth=cfg.depth,\n","        num_heads=cfg.num_heads,\n","        mlp_ratio=cfg.mlp_ratio,\n","        attn_type=cfg.attn_type,\n","        nb_features=cfg.nb_features,\n","        drop=cfg.drop,\n","        attn_drop=cfg.attn_drop,\n","        rpe_type=cfg.rpe_type,\n","    )\n","\n","# ================================================================\n","# 5. Training loop for MNIST / CIFAR-10 (simple baseline)\n","# ================================================================\n","import csv\n","import time\n","\n","def get_mnist_loaders(batch_size=128):\n","    # Basic preprocessing: resize to 32×32 then convert to tensor\n","    transform = T.Compose(\n","        [\n","            T.Resize(32),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    # Load MNIST train/test splits\n","    train_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=True, download=True, transform=transform\n","    )\n","    test_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=False, download=True, transform=transform\n","    )\n","\n","    # Wrap into DataLoader objects\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def get_cifar10_loaders(batch_size=128):\n","    # Data augmentation for training\n","    transform_train = T.Compose(\n","        [\n","            T.RandomHorizontalFlip(),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    # Standard preprocessing for test\n","    transform_test = T.Compose([T.ToTensor()])\n","\n","    # Load CIFAR-10 train/test splits\n","    train_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=True, download=True, transform=transform_train\n","    )\n","    test_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=False, download=True, transform=transform_test\n","    )\n","\n","    # Wrap into DataLoaders\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def train_one_epoch(model, loader, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","    for x, y in loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(x)  # forward pass\n","        loss = F.cross_entropy(logits, y)  # classification loss\n","        loss.backward()  # backprop\n","        optimizer.step()  # update weights\n","\n","        # Accumulate stats\n","        total_loss += loss.item() * x.size(0)\n","        preds = logits.argmax(dim=-1)\n","        total_correct += (preds == y).sum().item()\n","        total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","\n","    # Return average loss, accuracy, and elapsed time\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","def evaluate(model, loader, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            logits = model(x)\n","            loss = F.cross_entropy(logits, y)\n","\n","            total_loss += loss.item() * x.size(0)\n","            preds = logits.argmax(dim=-1)\n","            total_correct += (preds == y).sum().item()\n","            total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","# ================================================================\n","# CSV logging helper (appends without overwriting existing data)\n","# ================================================================\n","\n","def log_results(filepath, row_dict):\n","    file_exists = os.path.isfile(filepath)\n","    with open(filepath, \"a\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n","        if not file_exists:\n","            writer.writeheader()\n","        writer.writerow(row_dict)\n","\n","# ================================================================\n","# RUN EXAMPLE\n","# ================================================================\n","\n","def run_training_RPE(\n","    dataset: Literal[\"mnist\", \"cifar10\"] = \"mnist\",\n","    attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","    rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","    epochs: int = 5,\n","    lr: float = 1e-3,\n","    batch_size: int = 128,\n","):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    # 1) Dataset selection\n","    if dataset == \"mnist\":\n","        train_loader, test_loader = get_mnist_loaders(batch_size)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=1,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=attn_type,\n","            nb_features=64,\n","            rpe_type=rpe_type,\n","        )\n","\n","    elif dataset == \"cifar10\":\n","        train_loader, test_loader = get_cifar10_loaders(batch_size)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=3,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=attn_type,\n","            nb_features=64,\n","            rpe_type=rpe_type,\n","        )\n","\n","    # 2) Build model\n","    model = build_model(cfg).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    # 3) Training loop\n","    for epoch in range(1, epochs + 1):\n","        train_loss, train_acc, train_time = train_one_epoch(model, train_loader, optimizer, device)\n","        test_loss, test_acc, eval_time = evaluate(model, test_loader, device)\n","\n","        print(\n","            f\"[{dataset}][attn={attn_type}][rpe={rpe_type}] \"\n","            f\"Epoch {epoch:02d}: \"\n","            f\"train loss={train_loss:.4f}, train acc={train_acc:.4f}, \"\n","            f\"test loss={test_loss:.4f}, test acc={test_acc:.4f}\"\n","        )\n","\n","        # Log to CSV\n","        log_results(\n","            RESULTS_FILE,\n","            {\n","                \"dataset\": dataset,\n","                \"attn_type\": attn_type,\n","                \"rpe_type\": rpe_type,\n","                \"epochs_total\": epochs,\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": train_loss,\n","                \"train_acc\": train_acc,\n","                \"train_time_sec\": train_time,\n","                \"test_loss\": test_loss,\n","                \"test_acc\": test_acc,\n","                \"eval_time_sec\": eval_time,\n","            },\n","        )\n","\n","# datasets = [\"mnist\", \"cifar10\"]\n","datasets = [\"cifar10\"]\n","attn_types = [\"favor+\", \"relu\"]\n","rpe_types_full = [\"none\"]\n","rpe_types_performer = [\"none\", \"rope\", \"classic\", \"string\"]\n","\n","EPOCHS_MNIST = 5\n","EPOCHS_CIFAR = 20\n","\n","for dataset in datasets:\n","    for attn in attn_types:\n","        if attn == \"full\":\n","            to_run = rpe_types_full\n","        else:\n","            to_run = rpe_types_performer\n","\n","        for rpe in to_run:\n","            epochs = EPOCHS_MNIST if dataset == \"mnist\" else EPOCHS_CIFAR\n","\n","            print(\"\\n==============================================\")\n","            print(\n","                f\" RUN: dataset={dataset} | attn={attn} | rpe={rpe} | epochs={epochs}\"\n","            )\n","            print(\"==============================================\\n\")\n","\n","            run_training_RPE(\n","                dataset=dataset,\n","                attn_type=attn,\n","                rpe_type=rpe,\n","                epochs=epochs,\n","                lr=1e-3,\n","                batch_size=128,\n","            )"]},{"cell_type":"code","source":["!ls -l \"/content\"\n"],"metadata":{"id":"AAG8m_wkAml2","executionInfo":{"status":"aborted","timestamp":1765821859476,"user_tz":-60,"elapsed":33,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -l \"/content/drive/MyDrive/Colab Notebooks\"\n"],"metadata":{"id":"sk844iCoApTi","executionInfo":{"status":"aborted","timestamp":1765821859477,"user_tz":-60,"elapsed":34,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Special case used to maximize accuracy (for explanatory purposes)"],"metadata":{"id":"fWCZmUFeSJ4L"}},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"status":"ok","timestamp":1765821865197,"user_tz":-60,"elapsed":50,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}},"id":"TqvcK8lgop11"},"outputs":[],"source":["from torch.optim.lr_scheduler import CosineAnnealingLR\n","RESULTS_FILE_MAX = f\"{BASE_DIR}/Results/results_2_RPE_MAX_{TODAY}.csv\"\n","\n","# ================================================================\n","# 1. Patch embedding + CLS + simple learned absolute positions\n","# ================================================================\n","\n","class PatchEmbedding(nn.Module):\n","\n","    def __init__(\n","        self, img_size: int, patch_size: int, in_channels: int, embed_dim: int\n","    ):\n","        super().__init__()\n","        assert img_size % patch_size == 0,\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(\n","            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n","        )\n","\n","    def forward(self, x):\n","        x = self.proj(x)\n","        x = x.flatten(2)\n","        x = x.transpose(1, 2)\n","        return x\n","\n","\n","class ViTInputLayer(nn.Module):\n","    def __init__(self, img_size, patch_size, in_channels, embed_dim, cls_token=True):\n","        super().__init__()\n","\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","\n","        self.num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = (\n","            nn.Parameter(torch.zeros(1, 1, embed_dim)) if cls_token else None\n","        )\n","\n","    def forward(self, x):\n","        B = x.size(0)\n","        x = self.patch_embed(x)\n","\n","        if self.cls_token is not None:\n","            cls = self.cls_token.expand(B, -1, -1)\n","            x = torch.cat([cls, x], dim=1)\n","        return x\n","\n","# ================================================================\n","# 2. Multi-Head Attention variants\n","#    - Full softmax attention (baseline)\n","#    - Performer FAVOR+ (softmax approx with positive random features)\n","#    - Performer-ReLU (ReLU feature map)\n","# ================================================================\n","\n","class MultiHeadSelfAttentionFull(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.scale = self.head_dim**-0.5\n","\n","        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n","        qkv = qkv.permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n","        attn = F.softmax(attn_scores, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        out = torch.matmul(attn, v)\n","        out = out.transpose(1, 2).reshape(B, N, D)\n","        out = self.out_proj(out)\n","        return self.proj_drop(out)\n","\n","class PerformerAttentionFavorPlus(nn.Module):\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        nb_features: int = 64,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","        eps: float = 1e-6,\n","        rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","        seq_len: int | None = None,\n","    ):\n","        super().__init__()\n","\n","        assert embed_dim % num_heads == 0\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.nb_features = nb_features\n","        self.eps = eps\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        W = torch.randn(num_heads, nb_features, self.head_dim)\n","        self.register_buffer(\"W\", W)\n","\n","        self.rotary = None\n","        self.rpe_classic = None\n","        self.rpe_string = None\n","\n","        if rpe_type == \"rope\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for RoPE\")\n","            self.rotary = RotaryEmbedding(self.head_dim, max_seq_len=seq_len)\n","\n","        elif rpe_type == \"classic\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for classic RPE\")\n","            self.rpe_classic = ClassicRPEPerformer(seq_len)\n","\n","        elif rpe_type == \"string\":\n","            if seq_len is None:\n","                raise ValueError(\"seq_len required for STRING RPE\")\n","            self.rpe_string = CirculantSTRING(seq_len, self.head_dim)\n","\n","    def _favor_feature_map(self, x):\n","        B, H, N, d_k = x.shape\n","        x_proj = torch.einsum(\"b h n d, h m d -> b h n m\", x, self.W)\n","        sq_norm = (x**2).sum(dim=-1, keepdim=True) / 2.0\n","        return torch.exp(x_proj - sq_norm) / math.sqrt(self.nb_features)\n","\n","    def forward(self, x):\n","        B, N, D = x.shape\n","\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        k = k.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        v = v.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        if self.rotary is not None:\n","            q, k = self.rotary(q, k)\n","\n","        q_feat = self._favor_feature_map(q)\n","        k_feat = self._favor_feature_map(k)\n","\n","        kv = torch.einsum(\"b h n m, b h n d -> b h m d\", k_feat, v)\n","        k_sum = k_feat.sum(dim=2)\n","\n","        denom = torch.einsum(\"b h n m, b h m -> b h n\", q_feat, k_sum)\n","        denom = denom.unsqueeze(-1) + self.eps\n","\n","        out = torch.einsum(\"b h n m, b h m d -> b h n d\", q_feat, kv)\n","        out = out / denom\n","\n","        if self.rpe_classic is not None:\n","            weight = self.rpe_classic(N, x.device)\n","            out = torch.einsum(\"b h n d, n m -> b h m d\", out, weight)\n","\n","        if self.rpe_string is not None:\n","            weight = self.rpe_string(N, x.device)\n","            out = torch.einsum(\"b h n d, n m -> b h m d\", out, weight)\n","\n","        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","\n","# ================================================================\n","# 3. Transformer block + ViT backbone\n","# ================================================================\n","\n","class MLPBlock(nn.Module):\n","    def __init__(self, embed_dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n","        super().__init__()\n","        hidden_dim = int(embed_dim * mlp_ratio)\n","\n","        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","class TransformerEncoderBlock(nn.Module):\n","  def __init__(\n","      self,\n","      embed_dim: int,\n","      num_heads: int,\n","      mlp_ratio: float = 4.0,\n","      attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","      nb_features: int = 64,\n","      drop: float = 0.0,\n","      attn_drop: float = 0.0,\n","      rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","      seq_len: int | None = None,\n","  ):\n","      super().__init__()\n","\n","      self.norm1 = nn.LayerNorm(embed_dim)\n","      self.norm2 = nn.LayerNorm(embed_dim)\n","\n","      if attn_type == \"full\":\n","          self.attn = MultiHeadSelfAttentionFull(\n","              embed_dim,\n","              num_heads,\n","              attn_drop,\n","              drop,\n","          )\n","\n","      elif attn_type == \"favor+\":\n","          self.attn = PerformerAttentionFavorPlus(\n","              embed_dim,\n","              num_heads,\n","              nb_features,\n","              attn_drop,\n","              drop,\n","              rpe_type=rpe_type,\n","              seq_len=seq_len,\n","          )\n","      elif attn_type == \"relu\":\n","          self.attn = PerformerAttentionReLU(\n","              embed_dim,\n","              num_heads,\n","              attn_drop=attn_drop,\n","              proj_drop=drop,\n","              rpe_type=rpe_type,\n","              seq_len=seq_len,\n","          )\n","\n","      self.mlp = MLPBlock(embed_dim, mlp_ratio, drop)\n","\n","  def forward(self, x):\n","      x = x + self.attn(self.norm1(x))\n","      x = x + self.mlp(self.norm2(x))\n","      return x\n","\n","class ViTClassifier(nn.Module):\n","    def __init__(\n","        self,\n","        img_size: int,\n","        patch_size: int,\n","        in_channels: int,\n","        num_classes: int,\n","        embed_dim: int = 64,\n","        depth: int = 4,\n","        num_heads: int = 4,\n","        mlp_ratio: float = 4.0,\n","        attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","        nb_features: int = 64,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        rpe_type: Literal[\"none\", \"rope\", \"classic\", \"string\"] = \"none\",\n","    ):\n","        super().__init__()\n","\n","        self.input_layer = ViTInputLayer(\n","            img_size, patch_size, in_channels, embed_dim, cls_token=True\n","        )\n","\n","        seq_len = 1 + (img_size // patch_size) ** 2\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                TransformerEncoderBlock(\n","                    embed_dim=embed_dim,\n","                    num_heads=num_heads,\n","                    mlp_ratio=mlp_ratio,\n","                    attn_type=attn_type,\n","                    nb_features=nb_features,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    rpe_type=rpe_type,\n","                    seq_len=seq_len,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        x = self.input_layer(x)\n","        for blk in self.blocks:\n","            x = blk(x)\n","        x = self.norm(x)\n","        cls_token = x[:, 0]\n","        return self.head(cls_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8026640d-d379-4a3d-c8f9-330d2b321c67","id":"EXFW4M5Rop11"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","==============================================\n"," RUN: dataset=cifar10 | attn=favor+ | rpe=rope | epochs=60\n","==============================================\n","\n","Using device: cuda\n","[cifar10][attn=favor+][rpe=rope] Epoch 01: train loss=1.7617, train acc=0.3482, test loss=1.5459, test acc=0.4360\n","[cifar10][attn=favor+][rpe=rope] Epoch 02: train loss=1.5084, train acc=0.4496, test loss=1.4172, test acc=0.4897\n","[cifar10][attn=favor+][rpe=rope] Epoch 03: train loss=1.4252, train acc=0.4819, test loss=1.3808, test acc=0.4993\n","[cifar10][attn=favor+][rpe=rope] Epoch 04: train loss=1.3712, train acc=0.5050, test loss=1.3463, test acc=0.5136\n","[cifar10][attn=favor+][rpe=rope] Epoch 05: train loss=1.3258, train acc=0.5202, test loss=1.2983, test acc=0.5333\n","[cifar10][attn=favor+][rpe=rope] Epoch 06: train loss=1.2918, train acc=0.5352, test loss=1.2651, test acc=0.5410\n","[cifar10][attn=favor+][rpe=rope] Epoch 07: train loss=1.2570, train acc=0.5462, test loss=1.2244, test acc=0.5595\n","[cifar10][attn=favor+][rpe=rope] Epoch 08: train loss=1.2284, train acc=0.5567, test loss=1.2057, test acc=0.5674\n","[cifar10][attn=favor+][rpe=rope] Epoch 09: train loss=1.2069, train acc=0.5645, test loss=1.2067, test acc=0.5626\n","[cifar10][attn=favor+][rpe=rope] Epoch 10: train loss=1.1761, train acc=0.5765, test loss=1.1873, test acc=0.5713\n","[cifar10][attn=favor+][rpe=rope] Epoch 11: train loss=1.1561, train acc=0.5850, test loss=1.1725, test acc=0.5733\n","[cifar10][attn=favor+][rpe=rope] Epoch 12: train loss=1.1351, train acc=0.5913, test loss=1.1532, test acc=0.5883\n","[cifar10][attn=favor+][rpe=rope] Epoch 13: train loss=1.1187, train acc=0.5960, test loss=1.1439, test acc=0.5835\n","[cifar10][attn=favor+][rpe=rope] Epoch 14: train loss=1.1001, train acc=0.6055, test loss=1.1339, test acc=0.5916\n","[cifar10][attn=favor+][rpe=rope] Epoch 15: train loss=1.0792, train acc=0.6121, test loss=1.1250, test acc=0.5942\n","[cifar10][attn=favor+][rpe=rope] Epoch 16: train loss=1.0602, train acc=0.6193, test loss=1.1314, test acc=0.5959\n","[cifar10][attn=favor+][rpe=rope] Epoch 17: train loss=1.0463, train acc=0.6241, test loss=1.1051, test acc=0.6082\n","[cifar10][attn=favor+][rpe=rope] Epoch 18: train loss=1.0349, train acc=0.6301, test loss=1.0875, test acc=0.6066\n","[cifar10][attn=favor+][rpe=rope] Epoch 19: train loss=1.0145, train acc=0.6350, test loss=1.0847, test acc=0.6163\n","[cifar10][attn=favor+][rpe=rope] Epoch 20: train loss=1.0016, train acc=0.6410, test loss=1.0742, test acc=0.6177\n","[cifar10][attn=favor+][rpe=rope] Epoch 21: train loss=0.9826, train acc=0.6480, test loss=1.0680, test acc=0.6200\n","[cifar10][attn=favor+][rpe=rope] Epoch 22: train loss=0.9706, train acc=0.6517, test loss=1.0631, test acc=0.6218\n","[cifar10][attn=favor+][rpe=rope] Epoch 23: train loss=0.9549, train acc=0.6574, test loss=1.0407, test acc=0.6282\n","[cifar10][attn=favor+][rpe=rope] Epoch 24: train loss=0.9448, train acc=0.6612, test loss=1.0302, test acc=0.6333\n","[cifar10][attn=favor+][rpe=rope] Epoch 25: train loss=0.9283, train acc=0.6671, test loss=1.0431, test acc=0.6328\n","[cifar10][attn=favor+][rpe=rope] Epoch 26: train loss=0.9155, train acc=0.6733, test loss=1.0566, test acc=0.6291\n","[cifar10][attn=favor+][rpe=rope] Epoch 27: train loss=0.9020, train acc=0.6750, test loss=1.0271, test acc=0.6417\n","[cifar10][attn=favor+][rpe=rope] Epoch 28: train loss=0.8888, train acc=0.6830, test loss=1.0161, test acc=0.6427\n","[cifar10][attn=favor+][rpe=rope] Epoch 29: train loss=0.8687, train acc=0.6890, test loss=1.0163, test acc=0.6441\n","[cifar10][attn=favor+][rpe=rope] Epoch 30: train loss=0.8601, train acc=0.6918, test loss=1.0010, test acc=0.6524\n","[cifar10][attn=favor+][rpe=rope] Epoch 31: train loss=0.8474, train acc=0.6953, test loss=1.0129, test acc=0.6525\n","[cifar10][attn=favor+][rpe=rope] Epoch 32: train loss=0.8415, train acc=0.7007, test loss=0.9991, test acc=0.6543\n","[cifar10][attn=favor+][rpe=rope] Epoch 33: train loss=0.8193, train acc=0.7059, test loss=0.9913, test acc=0.6554\n","[cifar10][attn=favor+][rpe=rope] Epoch 34: train loss=0.8164, train acc=0.7086, test loss=0.9984, test acc=0.6589\n","[cifar10][attn=favor+][rpe=rope] Epoch 35: train loss=0.8020, train acc=0.7137, test loss=0.9930, test acc=0.6577\n","[cifar10][attn=favor+][rpe=rope] Epoch 36: train loss=0.7857, train acc=0.7188, test loss=0.9878, test acc=0.6549\n","[cifar10][attn=favor+][rpe=rope] Epoch 37: train loss=0.7809, train acc=0.7205, test loss=0.9835, test acc=0.6599\n","[cifar10][attn=favor+][rpe=rope] Epoch 38: train loss=0.7666, train acc=0.7245, test loss=0.9755, test acc=0.6651\n","[cifar10][attn=favor+][rpe=rope] Epoch 39: train loss=0.7617, train acc=0.7273, test loss=0.9922, test acc=0.6609\n","[cifar10][attn=favor+][rpe=rope] Epoch 40: train loss=0.7491, train acc=0.7296, test loss=0.9946, test acc=0.6614\n","[cifar10][attn=favor+][rpe=rope] Epoch 41: train loss=0.7385, train acc=0.7355, test loss=0.9861, test acc=0.6646\n","[cifar10][attn=favor+][rpe=rope] Epoch 42: train loss=0.7300, train acc=0.7371, test loss=0.9847, test acc=0.6673\n","[cifar10][attn=favor+][rpe=rope] Epoch 43: train loss=0.7219, train acc=0.7415, test loss=0.9569, test acc=0.6778\n","[cifar10][attn=favor+][rpe=rope] Epoch 44: train loss=0.7121, train acc=0.7440, test loss=0.9801, test acc=0.6697\n","[cifar10][attn=favor+][rpe=rope] Epoch 45: train loss=0.7006, train acc=0.7490, test loss=0.9859, test acc=0.6735\n","[cifar10][attn=favor+][rpe=rope] Epoch 46: train loss=0.6916, train acc=0.7498, test loss=0.9735, test acc=0.6692\n","[cifar10][attn=favor+][rpe=rope] Epoch 47: train loss=0.6876, train acc=0.7538, test loss=0.9718, test acc=0.6749\n","[cifar10][attn=favor+][rpe=rope] Epoch 48: train loss=0.6774, train acc=0.7565, test loss=0.9885, test acc=0.6717\n","[cifar10][attn=favor+][rpe=rope] Epoch 49: train loss=0.6738, train acc=0.7571, test loss=0.9939, test acc=0.6719\n","[cifar10][attn=favor+][rpe=rope] Epoch 50: train loss=0.6622, train acc=0.7635, test loss=0.9869, test acc=0.6751\n","[cifar10][attn=favor+][rpe=rope] Epoch 51: train loss=0.6495, train acc=0.7668, test loss=0.9629, test acc=0.6744\n","[cifar10][attn=favor+][rpe=rope] Epoch 52: train loss=0.6462, train acc=0.7682, test loss=0.9896, test acc=0.6782\n","[cifar10][attn=favor+][rpe=rope] Epoch 53: train loss=0.6416, train acc=0.7680, test loss=0.9801, test acc=0.6776\n","[cifar10][attn=favor+][rpe=rope] Epoch 54: train loss=0.6322, train acc=0.7737, test loss=0.9892, test acc=0.6762\n","[cifar10][attn=favor+][rpe=rope] Epoch 55: train loss=0.6264, train acc=0.7751, test loss=0.9837, test acc=0.6792\n","[cifar10][attn=favor+][rpe=rope] Epoch 56: train loss=0.6193, train acc=0.7763, test loss=0.9995, test acc=0.6740\n","[cifar10][attn=favor+][rpe=rope] Epoch 57: train loss=0.6120, train acc=0.7810, test loss=0.9997, test acc=0.6799\n","[cifar10][attn=favor+][rpe=rope] Epoch 58: train loss=0.5991, train acc=0.7835, test loss=1.0301, test acc=0.6736\n","[cifar10][attn=favor+][rpe=rope] Epoch 59: train loss=0.6003, train acc=0.7834, test loss=0.9978, test acc=0.6804\n"]}],"source":["# ---------------------------------------------------------\n","# RPE MODULES\n","# ---------------------------------------------------------\n","\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, head_dim: int, max_seq_len: int = 512):\n","        super().__init__()\n","        if head_dim % 2 != 0:\n","            raise ValueError(\"RoPE nécessite un head_dim pair\")\n","\n","        self.head_dim = head_dim\n","        self.max_seq_len = max_seq_len\n","\n","        inv_freq = 1.0 / (10000 ** (torch.arange(0, head_dim, 2).float() / head_dim))\n","        self.register_buffer(\"inv_freq\", inv_freq)\n","\n","    def _build_sin_cos(self, seq_len: int, device: torch.device):\n","        pos = torch.arange(seq_len, dtype=torch.float32, device=device)  # (N,)\n","        freqs = torch.einsum(\"n,d->nd\", pos, self.inv_freq)\n","\n","        sin = torch.sin(freqs)\n","        cos = torch.cos(freqs)\n","\n","        sin = torch.stack([sin, sin], dim=-1).reshape(seq_len, self.head_dim)\n","        cos = torch.stack([cos, cos], dim=-1).reshape(seq_len, self.head_dim)\n","\n","        return (\n","            sin.unsqueeze(0).unsqueeze(0),\n","            cos.unsqueeze(0).unsqueeze(0),\n","        )\n","\n","    @staticmethod\n","    def _rotate_half(x: torch.Tensor) -> torch.Tensor:\n","        d = x.shape[-1]\n","        x1 = x[..., : d // 2]\n","        x2 = x[..., d // 2 :]\n","        return torch.cat([-x2, x1], dim=-1)\n","\n","    def forward(self, q, k):\n","        B, H, N, d = q.shape\n","        if N > self.max_seq_len:\n","            raise ValueError(f\"seq_len {N} > max_seq_len {self.max_seq_len}\")\n","\n","        sin, cos = self._build_sin_cos(N, q.device)\n","\n","        q_rot = (q * cos) + (self._rotate_half(q) * sin)\n","        k_rot = (k * cos) + (self._rotate_half(k) * sin)\n","        return q_rot, k_rot\n","\n","# ================================================================\n","# 4. Helper functions to build model from config\n","# ================================================================\n","@dataclass\n","class ViTConfig:\n","    img_size: int = 32\n","    patch_size: int = 4\n","    in_channels: int = 3\n","    num_classes: int = 10\n","    embed_dim: int = 64\n","    depth: int = 4\n","    num_heads: int = 4\n","    mlp_ratio: float = 4.0\n","    attn_type: str = \"favor+\"\n","    nb_features: int = 64\n","    drop: float = 0.0\n","    attn_drop: float = 0.0\n","    rpe_type: Literal[\"rope\"] = \"rope\"\n","    drop: float = 0.1\n","    attn_drop: float = 0.1\n","\n","\n","\n","def build_model(cfg: ViTConfig) -> nn.Module:\n","    return ViTClassifier(\n","        img_size=cfg.img_size,\n","        patch_size=cfg.patch_size,\n","        in_channels=cfg.in_channels,\n","        num_classes=cfg.num_classes,\n","        embed_dim=cfg.embed_dim,\n","        depth=cfg.depth,\n","        num_heads=cfg.num_heads,\n","        mlp_ratio=cfg.mlp_ratio,\n","        attn_type=cfg.attn_type,\n","        nb_features=cfg.nb_features,\n","        drop=cfg.drop,\n","        attn_drop=cfg.attn_drop,\n","        rpe_type=cfg.rpe_type,\n","    )\n","\n","# ================================================================\n","# 5. Training loop for MNIST / CIFAR-10 (simple baseline)\n","# ================================================================\n","import csv\n","import time\n","\n","def get_mnist_loaders(batch_size=128):\n","    transform = T.Compose(\n","        [\n","            T.Resize(32),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    train_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=True, download=True, transform=transform\n","    )\n","    test_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=False, download=True, transform=transform\n","    )\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def get_cifar10_loaders(batch_size=128):\n","    transform_train = T.Compose(\n","        [\n","            T.RandomHorizontalFlip(),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    transform_test = T.Compose([T.ToTensor()])\n","\n","    train_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=True, download=True, transform=transform_train\n","    )\n","    test_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=False, download=True, transform=transform_test\n","    )\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def train_one_epoch(model, loader, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","    for x, y in loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(x)\n","        loss = F.cross_entropy(logits, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * x.size(0)\n","        preds = logits.argmax(dim=-1)\n","        total_correct += (preds == y).sum().item()\n","        total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","def evaluate(model, loader, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            logits = model(x)\n","            loss = F.cross_entropy(logits, y)\n","\n","            total_loss += loss.item() * x.size(0)\n","            preds = logits.argmax(dim=-1)\n","            total_correct += (preds == y).sum().item()\n","            total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","# ================================================================\n","# CSV logging helper (appends without overwriting existing data)\n","# ================================================================\n","\n","def log_results(filepath, row_dict):\n","    file_exists = os.path.isfile(filepath)\n","    with open(filepath, \"a\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n","        if not file_exists:\n","            writer.writeheader()\n","        writer.writerow(row_dict)\n","\n","# ================================================================\n","# RUN EXAMPLE\n","# ================================================================\n","\n","def run_training_RPE_maxperf(dataset=\"cifar10\"):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    epochs = 60 if dataset == \"cifar10\" else 10\n","\n","    if dataset == \"cifar10\":\n","        train_loader, test_loader = get_cifar10_loaders(128)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=3,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=\"favor+\",\n","            nb_features=64,\n","            rpe_type=\"rope\",\n","            drop=0.1,\n","            attn_drop=0.1,\n","        )\n","    else:\n","        train_loader, test_loader = get_mnist_loaders(128)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=1,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=\"favor+\",\n","            nb_features=64,\n","            rpe_type=\"rope\",\n","            drop=0.1,\n","            attn_drop=0.1,\n","        )\n","\n","    model = build_model(cfg).to(device)\n","\n","    optimizer = torch.optim.AdamW(\n","        model.parameters(),\n","        lr=3e-4,\n","        weight_decay=0.05\n","    )\n","\n","    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n","    warmup_epochs = 5\n","\n","    for epoch in range(1, epochs + 1):\n","\n","        if epoch <= warmup_epochs:\n","            lr_scale = epoch / warmup_epochs\n","            for pg in optimizer.param_groups:\n","                pg[\"lr\"] = lr_scale * 3e-4\n","\n","        train_loss, train_acc, _ = train_one_epoch(\n","            model, train_loader, optimizer, device\n","        )\n","        test_loss, test_acc, _ = evaluate(\n","            model, test_loader, device\n","        )\n","\n","        scheduler.step()\n","\n","        print(\n","            f\"[{dataset}] Epoch {epoch:03d} | \"\n","            f\"train acc={train_acc:.4f} | test acc={test_acc:.4f}\"\n","        )\n","\n","\n","        log_results(\n","            RESULTS_FILE_MAX,\n","            {\n","                \"dataset\": dataset,\n","                \"attn_type\": attn_type,\n","                \"rpe_type\": rpe_type,\n","                \"epochs_total\": epochs,\n","                \"epoch\": epoch,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": train_loss,\n","                \"train_acc\": train_acc,\n","                \"train_time_sec\": train_time,\n","                \"test_loss\": test_loss,\n","                \"test_acc\": test_acc,\n","                \"eval_time_sec\": eval_time,\n","            },\n","        )\n","\n","datasets = [\"cifar10\"]\n","attn_types = [\"favor+\"]\n","rpe_types_full = [\"none\"]\n","rpe_types_performer = [\"rope\"]\n","\n","EPOCHS_MNIST = 10\n","EPOCHS_CIFAR = 60\n","\n","for dataset in datasets:\n","    for attn in attn_types:\n","            to_run = rpe_types_performer\n","            for rpe in to_run:\n","                epochs = EPOCHS_MNIST if dataset == \"mnist\" else EPOCHS_CIFAR\n","\n","                print(\"\\n==============================================\")\n","                print(\n","                    f\" RUN: dataset={dataset} | attn={attn} | rpe={rpe} | epochs={epochs}\"\n","                )\n","                print(\"==============================================\\n\")\n","\n","                run_training_RPE(\n","                    dataset=dataset,\n","                    attn_type=attn,\n","                    rpe_type=rpe,\n","                    epochs=epochs,\n","                    lr=1e-3,\n","                    batch_size=128,\n","                )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}