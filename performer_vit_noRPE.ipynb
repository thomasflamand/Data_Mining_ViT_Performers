{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cH_Y1SxJgMKq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765819987039,"user_tz":-60,"elapsed":40095,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}},"outputId":"14b29502-fd5b-48ea-e2ae-6b21bea9ccf5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import math\n","from dataclasses import dataclass\n","from typing import Literal\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as T\n","\n","from google.colab import drive\n","\n","drive.mount(\"/content/drive\")\n","import pandas as pd\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eIgbr_bgXX9"},"outputs":[],"source":["BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Data_Mining\"\n","\n","CIFAR10_PATH = f\"{BASE_DIR}/cifar10\"\n","MNIST_PATH = f\"{BASE_DIR}/mnist\"\n","\n","TODAY = pd.Timestamp.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","RESULTS_FILE = f\"{BASE_DIR}/Results/results_2_noRPE_{TODAY}.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DH4bjtqQgMKt"},"outputs":[],"source":["# ================================================================\n","# 1. Patch embedding + CLS + simple learned absolute positions\n","# ================================================================\n","\n","\n","class PatchEmbedding(nn.Module):\n","    \"\"\"\n","    Image -> sequence of patch embeddings via a Conv2d layer.\n","    Args:\n","        img_size: size of the input image (assumed square)\n","        patch_size: size of each patch (assumed square)\n","        in_channels: number of input channels (e.g., 3 for RGB)\n","        embed_dim: dimension of the output embeddings\n","    \"\"\"\n","\n","    def __init__(\n","        self, img_size: int, patch_size: int, in_channels: int, embed_dim: int\n","    ):\n","        super().__init__()\n","        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = (img_size // patch_size) ** 2\n","        self.proj = nn.Conv2d(\n","            in_channels, embed_dim, kernel_size=patch_size, stride=patch_size\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: Tensor of shape (B, C, H, W)\n","                Input batch of images.\n","        Returns:\n","            Tensor of shape (B, N, D)\n","                Sequence of patch embeddings, where:\n","                    N = number of patches\n","                    D = embedding dimension.\n","        \"\"\"\n","        x = self.proj(x)\n","        x = x.flatten(2)\n","        x = x.transpose(1, 2)\n","        return x\n","\n","\n","class ViTInputLayer(nn.Module):\n","    \"\"\"\n","    PatchEmbedding + optional [CLS] token\n","\n","    This module converts an input image into a sequence of patch embeddings,\n","    optionally prepends a trainable [CLS] token, and adds learned positional\n","    encodings to all tokens.\n","    \"\"\"\n","\n","    def __init__(self, img_size, patch_size, in_channels, embed_dim, cls_token=True):\n","        super().__init__()\n","\n","        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n","\n","        self.num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = (\n","            nn.Parameter(torch.zeros(1, 1, embed_dim)) if cls_token else None\n","        )\n","\n","    def forward(self, x):\n","        B = x.size(0)\n","        x = self.patch_embed(x)\n","\n","        if self.cls_token is not None:\n","            cls = self.cls_token.expand(B, -1, -1)\n","            x = torch.cat([cls, x], dim=1)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQeIjky7gMKw"},"outputs":[],"source":["# ================================================================\n","# 2. Multi-Head Attention variants\n","#    - Full softmax attention (baseline)\n","#    - Performer FAVOR+ (softmax approx with positive random features)\n","#    - Performer-ReLU (ReLU feature map)\n","# ================================================================\n","\n","\n","class MultiHeadSelfAttentionFull(nn.Module):\n","    \"\"\"\n","    Standard multi-head self-attention (softmax attention).\n","    Baseline attention used in the original ViT.\n","    No kernel approximation.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0\n","\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","\n","        self.qkv = nn.Linear(embed_dim, 3 * embed_dim)\n","\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, N, D)\n","        Returns: (B, N, D)\n","        \"\"\"\n","        B, N, D = x.shape\n","\n","        qkv = self.qkv(x)\n","\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim)\n","\n","        qkv = qkv.permute(2, 0, 3, 1, 4)\n","\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        scale = 1.0 / math.sqrt(self.head_dim)\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n","\n","        attn = F.softmax(attn_scores, dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        out = torch.matmul(attn, v)\n","\n","        out = out.transpose(1, 2).reshape(B, N, D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","\n","class PerformerAttentionFavorPlus(nn.Module):\n","    \"\"\"\n","    Performer FAVOR+ : approximates exp(q k^T) using positive random features.\n","    Complexity O(N * m) instead of O(N^2).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        nb_features: int = 64,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","        eps: float = 1e-6,\n","    ):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.nb_features = nb_features\n","        self.eps = eps\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.register_buffer(\"W\", torch.randn(num_heads, nb_features, self.head_dim))\n","\n","    def _favor_feature_map(self, x):\n","        \"\"\"\n","        x: (B, H, N, d_k)\n","        -> phi(x): (B, H, N, m)\n","        phi(x) = exp(Wx - ||x||^2 / 2) / sqrt(m)\n","        \"\"\"\n","        B, H, N, d_k = x.shape\n","\n","        x_proj = torch.einsum(\"b h n d, h m d -> b h n m\", x, self.W)\n","\n","        sq_norm = (x**2).sum(dim=-1, keepdim=True) / 2.0\n","\n","        x_feat = torch.exp(x_proj - sq_norm) / math.sqrt(self.nb_features)\n","        return x_feat\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, N, D)\n","        \"\"\"\n","        B, N, D = x.shape\n","\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        k = k.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        v = v.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        q_feat = self._favor_feature_map(q)\n","        k_feat = self._favor_feature_map(k)\n","\n","        kv = torch.einsum(\"b h n m, b h n d -> b h m d\", k_feat, v)\n","\n","        k_sum = k_feat.sum(dim=2)\n","\n","        denom = torch.einsum(\"b h n m, b h m -> b h n\", q_feat, k_sum)\n","        denom = denom.unsqueeze(-1) + self.eps\n","\n","        out = torch.einsum(\"b h n m, b h m d -> b h n d\", q_feat, kv)\n","        out = out / denom\n","        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","\n","class PerformerAttentionReLU(nn.Module):\n","    \"\"\"\n","    Performer-ReLU : kernel phi(x) = ReLU(x).\n","    Same linear-attention scheme as FAVOR+, but no random features.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","        eps: float = 1e-6,\n","    ):\n","        super().__init__()\n","        assert embed_dim % num_heads == 0\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","        self.eps = eps\n","\n","        self.q_proj = nn.Linear(embed_dim, embed_dim)\n","        self.k_proj = nn.Linear(embed_dim, embed_dim)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim)\n","\n","        self.out_proj = nn.Linear(embed_dim, embed_dim)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def _relu_feature_map(self, x):\n","        \"\"\"\n","        x: (B, H, N, d_k)\n","        Feature map phi(x) = ReLU(x) ensures non-negativity.\n","        \"\"\"\n","        return F.relu(x)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, N, D)\n","        \"\"\"\n","        B, N, D = x.shape\n","\n","        q = self.q_proj(x)\n","        k = self.k_proj(x)\n","        v = self.v_proj(x)\n","\n","        q = q.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        k = k.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","        v = v.reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        q_feat = self._relu_feature_map(q)\n","        k_feat = self._relu_feature_map(k)\n","\n","        kv = torch.einsum(\"b h n d, b h n e -> b h d e\", k_feat, v)\n","\n","        k_sum = k_feat.sum(dim=2)\n","\n","        denom = torch.einsum(\"b h n d, b h d -> b h n\", q_feat, k_sum)\n","        denom = denom.unsqueeze(-1) + self.eps\n","\n","        out = torch.einsum(\"b h n d, b h d e -> b h n e\", q_feat, kv)\n","\n","        out = out / denom\n","\n","        out = out.permute(0, 2, 1, 3).reshape(B, N, D)\n","\n","        out = self.out_proj(out)\n","        out = self.proj_drop(out)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUuqmoaIgMKz"},"outputs":[],"source":["# ================================================================\n","# 3. Transformer block + ViT backbone\n","# ================================================================\n","\n","\n","class MLPBlock(nn.Module):\n","    \"\"\"\n","    Standard Transformer feedforward block:\n","    Linear -> GELU -> Linear (with optional dropout).\n","\n","    Position-wise feedforward network that expands and compresses each token\n","    to increase the transformer's expressive capacity.\n","    \"\"\"\n","\n","    def __init__(self, embed_dim: int, mlp_ratio: float = 4.0, drop: float = 0.0):\n","        super().__init__()\n","        hidden_dim = int(embed_dim * mlp_ratio)\n","\n","        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class TransformerEncoderBlock(nn.Module):\n","    \"\"\"\n","    Pre-LN Transformer block with choice of attention mechanism.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        embed_dim: int,\n","        num_heads: int,\n","        mlp_ratio: float = 4.0,\n","        attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","        nb_features: int = 64,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","    ):\n","        super().__init__()\n","\n","        self.norm1 = nn.LayerNorm(embed_dim)\n","        self.norm2 = nn.LayerNorm(embed_dim)\n","\n","        if attn_type == \"full\":\n","            self.attn = MultiHeadSelfAttentionFull(\n","                embed_dim, num_heads, attn_drop, drop\n","            )\n","        elif attn_type == \"favor+\":\n","            self.attn = PerformerAttentionFavorPlus(\n","                embed_dim, num_heads, nb_features, attn_drop, drop\n","            )\n","        elif attn_type == \"relu\":\n","            self.attn = PerformerAttentionReLU(embed_dim, num_heads, attn_drop, drop)\n","        else:\n","            raise ValueError(f\"Unknown attn_type: {attn_type}\")\n","\n","        self.mlp = MLPBlock(embed_dim, mlp_ratio, drop)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.norm1(x))\n","\n","        x = x + self.mlp(self.norm2(x))\n","        return x\n","\n","\n","class ViTClassifier(nn.Module):\n","    \"\"\"\n","    Vision Transformer / Performer-ViT classifier for MNIST or CIFAR-10.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        img_size: int,\n","        patch_size: int,\n","        in_channels: int,\n","        num_classes: int,\n","        embed_dim: int = 64,\n","        depth: int = 4,\n","        num_heads: int = 4,\n","        mlp_ratio: float = 4.0,\n","        attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"full\",\n","        nb_features: int = 64,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","    ):\n","        super().__init__()\n","\n","        self.input_layer = ViTInputLayer(\n","            img_size, patch_size, in_channels, embed_dim, cls_token=True\n","        )\n","\n","        self.blocks = nn.ModuleList(\n","            [\n","                TransformerEncoderBlock(\n","                    embed_dim=embed_dim,\n","                    num_heads=num_heads,\n","                    mlp_ratio=mlp_ratio,\n","                    attn_type=attn_type,\n","                    nb_features=nb_features,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                )\n","                for _ in range(depth)\n","            ]\n","        )\n","\n","        self.norm = nn.LayerNorm(embed_dim)\n","\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (B, C, H, W)\n","        \"\"\"\n","        x = self.input_layer(x)\n","\n","        for blk in self.blocks:\n","            x = blk(x)\n","\n","        x = self.norm(x)\n","\n","        cls_token = x[:, 0]\n","\n","        logits = self.head(cls_token)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4XD0eHZEgMK0"},"outputs":[],"source":["# ================================================================\n","# 4. Helper functions to build model from config\n","# ================================================================\n","\n","\n","@dataclass\n","class ViTConfig:\n","    \"\"\"\n","    Configuration container for building a ViT/Performer-ViT model.\n","    \"\"\"\n","\n","    img_size: int = 32\n","    patch_size: int = 4\n","    in_channels: int = 3\n","    num_classes: int = 10\n","    embed_dim: int = 64\n","    depth: int = 4\n","    num_heads: int = 4\n","    mlp_ratio: float = 4.0\n","    attn_type: str = \"full\"\n","    nb_features: int = 64\n","    drop: float = 0.0\n","    attn_drop: float = 0.0\n","\n","\n","def build_model(cfg: ViTConfig) -> nn.Module:\n","    \"\"\"\n","    Build a ViT/Performer-ViT model from a configuration object.\n","    \"\"\"\n","    return ViTClassifier(\n","        img_size=cfg.img_size,\n","        patch_size=cfg.patch_size,\n","        in_channels=cfg.in_channels,\n","        num_classes=cfg.num_classes,\n","        embed_dim=cfg.embed_dim,\n","        depth=cfg.depth,\n","        num_heads=cfg.num_heads,\n","        mlp_ratio=cfg.mlp_ratio,\n","        attn_type=cfg.attn_type,\n","        nb_features=cfg.nb_features,\n","        drop=cfg.drop,\n","        attn_drop=cfg.attn_drop,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYJLzvMogMK1"},"outputs":[],"source":["# ================================================================\n","# 5. Training loop for MNIST / CIFAR-10 (simple baseline)\n","# ================================================================\n","import csv\n","import time\n","\n","def get_mnist_loaders(batch_size=128):\n","    transform = T.Compose(\n","        [\n","            T.Resize(32),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    train_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=True, download=False, transform=transform\n","    )\n","    test_set = torchvision.datasets.MNIST(\n","        root=MNIST_PATH, train=False, download=False, transform=transform\n","    )\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def get_cifar10_loaders(batch_size=128):\n","    transform_train = T.Compose(\n","        [\n","            T.RandomHorizontalFlip(),\n","            T.ToTensor(),\n","        ]\n","    )\n","\n","    transform_test = T.Compose([T.ToTensor()])\n","\n","    train_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=True, download=False, transform=transform_train\n","    )\n","    test_set = torchvision.datasets.CIFAR10(\n","        root=CIFAR10_PATH, train=False, download=False, transform=transform_test\n","    )\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n","    return train_loader, test_loader\n","\n","\n","def train_one_epoch(model, loader, optimizer, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","    for x, y in loader:\n","        x = x.to(device)\n","        y = y.to(device)\n","\n","        optimizer.zero_grad()\n","        logits = model(x)\n","        loss = F.cross_entropy(logits, y)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * x.size(0)\n","        preds = logits.argmax(dim=-1)\n","        total_correct += (preds == y).sum().item()\n","        total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","def evaluate(model, loader, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    total_samples = 0\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device)\n","            y = y.to(device)\n","\n","            logits = model(x)\n","            loss = F.cross_entropy(logits, y)\n","\n","            total_loss += loss.item() * x.size(0)\n","            preds = logits.argmax(dim=-1)\n","            total_correct += (preds == y).sum().item()\n","            total_samples += x.size(0)\n","\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    return total_loss / total_samples, total_correct / total_samples, elapsed_time\n","\n","\n","# ================================================================\n","# CSV logging helper (appends without overwriting existing data)\n","# ================================================================\n","\n","\n","def log_results(filepath, row_dict):\n","    file_exists = os.path.isfile(filepath)\n","    with open(filepath, \"a\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n","        if not file_exists:\n","            writer.writeheader()\n","        writer.writerow(row_dict)\n","\n","\n","# ================================================================\n","# RUN EXAMPLE\n","# ================================================================\n","def run_training_noRPE(\n","    dataset: Literal[\"mnist\", \"cifar10\"] = \"mnist\",\n","    attn_type: Literal[\"full\", \"favor+\", \"relu\"] = \"favor+\",\n","    epochs: int = 5,\n","    lr: float = 1e-3,\n","    batch_size: int = 128,\n","):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    if dataset == \"mnist\":\n","        train_loader, test_loader = get_mnist_loaders(batch_size)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=1,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=attn_type,\n","            nb_features=64,\n","        )\n","\n","    elif dataset == \"cifar10\":\n","        train_loader, test_loader = get_cifar10_loaders(batch_size)\n","        cfg = ViTConfig(\n","            img_size=32,\n","            patch_size=4,\n","            in_channels=3,\n","            num_classes=10,\n","            embed_dim=64,\n","            depth=4,\n","            num_heads=4,\n","            mlp_ratio=4.0,\n","            attn_type=attn_type,\n","            nb_features=64,\n","        )\n","\n","    else:\n","        raise ValueError(\"dataset must be 'mnist' or 'cifar10'\")\n","\n","    model = build_model(cfg).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    for epoch in range(1, epochs + 1):\n","        train_loss, train_acc, train_time = train_one_epoch(model, train_loader, optimizer, device)\n","        test_loss, test_acc, eval_time = evaluate(model, test_loader, device)\n","\n","        print(\n","            f\"[{dataset}][{attn_type}] \"\n","            f\"Epoch {epoch:02d}: \"\n","            f\"train loss={train_loss:.4f}, train acc={train_acc:.4f}, train time={train_time:.2f}s, \"\n","            f\"test loss={test_loss:.4f}, test acc={test_acc:.4f}, eval time={eval_time:.2f}s\"\n","        )\n","\n","        log_results(\n","            RESULTS_FILE,\n","            {\n","                \"dataset\": dataset,\n","                \"attn_type\": attn_type,\n","                \"epoch\": epoch,\n","                \"epochs_total\": epochs,\n","                \"lr\": lr,\n","                \"batch_size\": batch_size,\n","                \"train_loss\": train_loss,\n","                \"train_acc\": train_acc,\n","                \"train_time_sec\": train_time,\n","                \"test_loss\": test_loss,\n","                \"test_acc\": test_acc,\n","                \"eval_time_sec\": eval_time,\n","            },\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQc70qKcpqDT","outputId":"bc786123-e2bf-41bb-f612-e85f13b804e4","executionInfo":{"status":"ok","timestamp":1765819987654,"user_tz":-60,"elapsed":39,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec 15 17:33:07 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhffXsG6gMK2","outputId":"928ed85e-53eb-411f-8545-25b9244bfb52","executionInfo":{"status":"ok","timestamp":1765821392962,"user_tz":-60,"elapsed":432538,"user":{"displayName":"Thomas Flamand","userId":"13869132923269292623"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==============================================\n"," RUN (no RPE): dataset=mnist | attn=full | epochs=5\n","==============================================\n","\n","Using device: cuda\n","[mnist][full] Epoch 01: train loss=1.5229, train acc=0.4350, train time=17.84s, test loss=1.0315, test acc=0.6315, eval time=2.05s\n","[mnist][full] Epoch 02: train loss=0.8520, train acc=0.7037, train time=16.65s, test loss=0.6763, test acc=0.7653, eval time=1.81s\n","[mnist][full] Epoch 03: train loss=0.5927, train acc=0.7986, train time=15.85s, test loss=0.5264, test acc=0.8216, eval time=1.78s\n","[mnist][full] Epoch 04: train loss=0.4753, train acc=0.8399, train time=16.05s, test loss=0.4444, test acc=0.8485, eval time=2.60s\n","[mnist][full] Epoch 05: train loss=0.4072, train acc=0.8618, train time=15.73s, test loss=0.3715, test acc=0.8756, eval time=1.78s\n","\n","==============================================\n"," RUN (no RPE): dataset=mnist | attn=favor+ | epochs=5\n","==============================================\n","\n","Using device: cuda\n","[mnist][favor+] Epoch 01: train loss=1.3274, train acc=0.5193, train time=19.55s, test loss=0.9296, test acc=0.6704, eval time=2.61s\n","[mnist][favor+] Epoch 02: train loss=0.7456, train acc=0.7440, train time=19.15s, test loss=0.6226, test acc=0.7815, eval time=2.10s\n","[mnist][favor+] Epoch 03: train loss=0.5713, train acc=0.8061, train time=19.78s, test loss=0.5133, test acc=0.8319, eval time=2.08s\n","[mnist][favor+] Epoch 04: train loss=0.4863, train acc=0.8361, train time=19.52s, test loss=0.4995, test acc=0.8348, eval time=2.47s\n","[mnist][favor+] Epoch 05: train loss=0.4177, train acc=0.8592, train time=19.17s, test loss=0.3966, test acc=0.8662, eval time=2.07s\n","\n","==============================================\n"," RUN (no RPE): dataset=mnist | attn=relu | epochs=5\n","==============================================\n","\n","Using device: cuda\n","[mnist][relu] Epoch 01: train loss=1.4382, train acc=0.4625, train time=17.29s, test loss=1.0909, test acc=0.6048, eval time=1.89s\n","[mnist][relu] Epoch 02: train loss=0.8286, train acc=0.7133, train time=16.22s, test loss=0.7090, test acc=0.7571, eval time=1.86s\n","[mnist][relu] Epoch 03: train loss=0.5880, train acc=0.8010, train time=16.75s, test loss=0.5592, test acc=0.8089, eval time=2.30s\n","[mnist][relu] Epoch 04: train loss=0.4659, train acc=0.8441, train time=16.37s, test loss=0.3735, test acc=0.8735, eval time=1.85s\n","[mnist][relu] Epoch 05: train loss=0.3951, train acc=0.8652, train time=16.23s, test loss=0.3386, test acc=0.8899, eval time=2.55s\n","\n","==============================================\n"," RUN (no RPE): dataset=cifar10 | attn=full | epochs=20\n","==============================================\n","\n","Using device: cuda\n","[cifar10][full] Epoch 01: train loss=1.8380, train acc=0.3098, train time=15.44s, test loss=1.6895, test acc=0.3766, eval time=2.38s\n","[cifar10][full] Epoch 02: train loss=1.5650, train acc=0.4266, train time=15.00s, test loss=1.4967, test acc=0.4530, eval time=1.67s\n","[cifar10][full] Epoch 03: train loss=1.4629, train acc=0.4671, train time=15.01s, test loss=1.4580, test acc=0.4713, eval time=1.67s\n","[cifar10][full] Epoch 04: train loss=1.3926, train acc=0.4918, train time=14.91s, test loss=1.4457, test acc=0.4820, eval time=2.25s\n","[cifar10][full] Epoch 05: train loss=1.3424, train acc=0.5138, train time=15.40s, test loss=1.3745, test acc=0.5089, eval time=1.67s\n","[cifar10][full] Epoch 06: train loss=1.3116, train acc=0.5247, train time=14.95s, test loss=1.3223, test acc=0.5212, eval time=1.70s\n","[cifar10][full] Epoch 07: train loss=1.2802, train acc=0.5372, train time=14.99s, test loss=1.3001, test acc=0.5330, eval time=1.94s\n","[cifar10][full] Epoch 08: train loss=1.2451, train acc=0.5530, train time=15.46s, test loss=1.2906, test acc=0.5419, eval time=1.68s\n","[cifar10][full] Epoch 09: train loss=1.2215, train acc=0.5587, train time=15.01s, test loss=1.2589, test acc=0.5450, eval time=1.68s\n","[cifar10][full] Epoch 10: train loss=1.1959, train acc=0.5681, train time=14.94s, test loss=1.2650, test acc=0.5465, eval time=1.73s\n","[cifar10][full] Epoch 11: train loss=1.1733, train acc=0.5763, train time=15.49s, test loss=1.2626, test acc=0.5454, eval time=1.99s\n","[cifar10][full] Epoch 12: train loss=1.1541, train acc=0.5844, train time=14.98s, test loss=1.2176, test acc=0.5600, eval time=1.69s\n","[cifar10][full] Epoch 13: train loss=1.1306, train acc=0.5914, train time=14.86s, test loss=1.2428, test acc=0.5559, eval time=1.68s\n","[cifar10][full] Epoch 14: train loss=1.1101, train acc=0.5984, train time=15.16s, test loss=1.2178, test acc=0.5653, eval time=2.26s\n","[cifar10][full] Epoch 15: train loss=1.0948, train acc=0.6033, train time=14.87s, test loss=1.2377, test acc=0.5592, eval time=1.70s\n","[cifar10][full] Epoch 16: train loss=1.0756, train acc=0.6093, train time=14.95s, test loss=1.2097, test acc=0.5707, eval time=1.64s\n","[cifar10][full] Epoch 17: train loss=1.0533, train acc=0.6207, train time=14.79s, test loss=1.2203, test acc=0.5674, eval time=2.16s\n","[cifar10][full] Epoch 18: train loss=1.0340, train acc=0.6262, train time=15.27s, test loss=1.2152, test acc=0.5680, eval time=1.64s\n","[cifar10][full] Epoch 19: train loss=1.0137, train acc=0.6358, train time=14.93s, test loss=1.2121, test acc=0.5712, eval time=1.68s\n","[cifar10][full] Epoch 20: train loss=0.9939, train acc=0.6420, train time=14.93s, test loss=1.2307, test acc=0.5693, eval time=1.73s\n","\n","==============================================\n"," RUN (no RPE): dataset=cifar10 | attn=favor+ | epochs=20\n","==============================================\n","\n","Using device: cuda\n","[cifar10][favor+] Epoch 01: train loss=1.8497, train acc=0.3039, train time=17.81s, test loss=1.6859, test acc=0.3779, eval time=1.92s\n","[cifar10][favor+] Epoch 02: train loss=1.6110, train acc=0.4022, train time=17.44s, test loss=1.5590, test acc=0.4291, eval time=2.53s\n","[cifar10][favor+] Epoch 03: train loss=1.5215, train acc=0.4428, train time=17.52s, test loss=1.4939, test acc=0.4531, eval time=1.87s\n","[cifar10][favor+] Epoch 04: train loss=1.4548, train acc=0.4688, train time=17.62s, test loss=1.4479, test acc=0.4726, eval time=2.46s\n","[cifar10][favor+] Epoch 05: train loss=1.4097, train acc=0.4882, train time=17.38s, test loss=1.4410, test acc=0.4797, eval time=1.86s\n","[cifar10][favor+] Epoch 06: train loss=1.3697, train acc=0.5036, train time=17.86s, test loss=1.3694, test acc=0.5040, eval time=2.24s\n","[cifar10][favor+] Epoch 07: train loss=1.3398, train acc=0.5115, train time=17.39s, test loss=1.3895, test acc=0.5011, eval time=1.90s\n","[cifar10][favor+] Epoch 08: train loss=1.3136, train acc=0.5204, train time=18.38s, test loss=1.3676, test acc=0.5058, eval time=1.94s\n","[cifar10][favor+] Epoch 09: train loss=1.2831, train acc=0.5357, train time=17.38s, test loss=1.3343, test acc=0.5176, eval time=1.92s\n","[cifar10][favor+] Epoch 10: train loss=1.2611, train acc=0.5416, train time=18.08s, test loss=1.3131, test acc=0.5206, eval time=1.89s\n","[cifar10][favor+] Epoch 11: train loss=1.2411, train acc=0.5507, train time=17.34s, test loss=1.3239, test acc=0.5169, eval time=1.90s\n","[cifar10][favor+] Epoch 12: train loss=1.2263, train acc=0.5569, train time=18.23s, test loss=1.3090, test acc=0.5305, eval time=1.90s\n","[cifar10][favor+] Epoch 13: train loss=1.2049, train acc=0.5630, train time=17.45s, test loss=1.2808, test acc=0.5357, eval time=1.90s\n","[cifar10][favor+] Epoch 14: train loss=1.1847, train acc=0.5700, train time=18.17s, test loss=1.3012, test acc=0.5339, eval time=1.88s\n","[cifar10][favor+] Epoch 15: train loss=1.1667, train acc=0.5791, train time=17.53s, test loss=1.2541, test acc=0.5508, eval time=2.08s\n","[cifar10][favor+] Epoch 16: train loss=1.1440, train acc=0.5854, train time=18.06s, test loss=1.2278, test acc=0.5551, eval time=1.94s\n","[cifar10][favor+] Epoch 17: train loss=1.1273, train acc=0.5901, train time=17.44s, test loss=1.2446, test acc=0.5526, eval time=2.29s\n","[cifar10][favor+] Epoch 18: train loss=1.1160, train acc=0.5977, train time=17.85s, test loss=1.2417, test acc=0.5589, eval time=1.89s\n","[cifar10][favor+] Epoch 19: train loss=1.0945, train acc=0.6053, train time=17.42s, test loss=1.2439, test acc=0.5545, eval time=2.43s\n","[cifar10][favor+] Epoch 20: train loss=1.0782, train acc=0.6092, train time=17.81s, test loss=1.2551, test acc=0.5524, eval time=1.91s\n","\n","==============================================\n"," RUN (no RPE): dataset=cifar10 | attn=relu | epochs=20\n","==============================================\n","\n","Using device: cuda\n","[cifar10][relu] Epoch 01: train loss=1.9247, train acc=0.2707, train time=15.43s, test loss=1.7927, test acc=0.3202, eval time=2.34s\n","[cifar10][relu] Epoch 02: train loss=1.7234, train acc=0.3504, train time=15.58s, test loss=1.7192, test acc=0.3601, eval time=1.74s\n","[cifar10][relu] Epoch 03: train loss=1.6097, train acc=0.4061, train time=15.28s, test loss=1.5796, test acc=0.4124, eval time=1.74s\n","[cifar10][relu] Epoch 04: train loss=1.5218, train acc=0.4413, train time=15.37s, test loss=1.5184, test acc=0.4385, eval time=2.34s\n","[cifar10][relu] Epoch 05: train loss=1.4553, train acc=0.4667, train time=15.48s, test loss=1.4518, test acc=0.4650, eval time=1.68s\n","[cifar10][relu] Epoch 06: train loss=1.4129, train acc=0.4867, train time=15.25s, test loss=1.4048, test acc=0.4869, eval time=1.72s\n","[cifar10][relu] Epoch 07: train loss=1.3817, train acc=0.4964, train time=15.54s, test loss=1.3870, test acc=0.4960, eval time=2.36s\n","[cifar10][relu] Epoch 08: train loss=1.3494, train acc=0.5105, train time=15.42s, test loss=1.3622, test acc=0.5062, eval time=1.70s\n","[cifar10][relu] Epoch 09: train loss=1.3221, train acc=0.5193, train time=15.42s, test loss=1.3638, test acc=0.5017, eval time=1.67s\n","[cifar10][relu] Epoch 10: train loss=1.4185, train acc=0.4840, train time=15.47s, test loss=1.7830, test acc=0.3449, eval time=2.37s\n","[cifar10][relu] Epoch 11: train loss=1.6335, train acc=0.4027, train time=15.13s, test loss=1.5606, test acc=0.4361, eval time=1.74s\n","[cifar10][relu] Epoch 12: train loss=1.4542, train acc=0.4723, train time=15.19s, test loss=1.4840, test acc=0.4635, eval time=1.69s\n","[cifar10][relu] Epoch 13: train loss=1.3809, train acc=0.5002, train time=15.28s, test loss=1.3916, test acc=0.5029, eval time=2.35s\n","[cifar10][relu] Epoch 14: train loss=1.3230, train acc=0.5232, train time=15.19s, test loss=1.3446, test acc=0.5124, eval time=1.68s\n","[cifar10][relu] Epoch 15: train loss=1.2792, train acc=0.5377, train time=15.21s, test loss=1.3146, test acc=0.5253, eval time=1.66s\n","[cifar10][relu] Epoch 16: train loss=1.2514, train acc=0.5460, train time=15.29s, test loss=1.3006, test acc=0.5310, eval time=2.31s\n","[cifar10][relu] Epoch 17: train loss=1.2273, train acc=0.5561, train time=15.44s, test loss=1.2880, test acc=0.5341, eval time=1.68s\n","[cifar10][relu] Epoch 18: train loss=1.1990, train acc=0.5649, train time=15.11s, test loss=1.2875, test acc=0.5402, eval time=1.74s\n","[cifar10][relu] Epoch 19: train loss=1.1812, train acc=0.5721, train time=15.41s, test loss=1.2645, test acc=0.5473, eval time=2.35s\n","[cifar10][relu] Epoch 20: train loss=1.1589, train acc=0.5805, train time=15.37s, test loss=1.2543, test acc=0.5449, eval time=1.68s\n"]}],"source":["EPOCHS_MNIST = 5\n","EPOCHS_CIFAR = 20\n","\n","\n","if __name__ == \"__main__\":\n","    for dataset in [\"mnist\",\"cifar10\"]:\n","        for attn in [\"full\",\"favor+\", \"relu\"]:\n","            epochs = EPOCHS_MNIST if dataset == \"mnist\" else EPOCHS_CIFAR\n","\n","            print(\"\\n==============================================\")\n","            print(\n","                f\" RUN (no RPE): dataset={dataset} | attn={attn} | epochs={epochs}\"\n","            )\n","            print(\"==============================================\\n\")\n","\n","            run_training_noRPE(\n","                dataset=dataset,\n","                attn_type=attn,\n","                epochs=epochs,\n","            )"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":0}